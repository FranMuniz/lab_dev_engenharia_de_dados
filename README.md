## üíª Laborat√≥rio de Desenvolvimento - Engenharia de Dados

Um laborat√≥rio completo para estudos em **Engenharia de Dados**, rodando via **Docker**.  
Aqui voc√™ encontra um ambiente integrado com **Spark, PySpark, Jupyter, PostgreSQL, MinIO, Metabase e Airflow**, pronto para treinar desde consultas SQL at√© orquestra√ß√£o de pipelines de dados, armazenamento S3 local e visualiza√ß√£o.
Sinta-se √† vontade para utilizar o lab e explorar todos os servi√ßos!


<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=mysql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/PySpark-EE4C2C?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark_SQL-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/MinIO-26A69A?style=for-the-badge&logo=minio&logoColor=white"/>
  <img src="https://img.shields.io/badge/Postgres-316192?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white"/>
  <img src="https://img.shields.io/badge/Metabase-509EE3?style=for-the-badge&logo=metabase&logoColor=white"/>
</p>

---

### Pr√©-requisitos

Antes de iniciar, certifique-se de que sua m√°quina possui:

- **Docker** (recomendado vers√£o 20+): para rodar os containers.
- **Docker Compose** (vers√£o 1.27+ ou integrada no Docker Desktop): para subir todos os servi√ßos do lab com um √∫nico comando.
- **Git**: para clonar o reposit√≥rio.
- **Python 3.9+** (opcional, caso queira gerar a chave Fernet ou testar scripts fora do container).
- **PostgreSQL**: √© recomendado criar **schemas separados** para cada servi√ßo (por exemplo, `airflow` e `metabase`) para evitar conflito de tabelas internas.

Exemplo de cria√ß√£o de schemas:
```sql
-- Schema para Airflow
CREATE SCHEMA IF NOT EXISTS airflow;
GRANT ALL PRIVILEGES ON SCHEMA airflow TO admin;

-- Schema para Metabase
CREATE SCHEMA IF NOT EXISTS metabase;
GRANT ALL PRIVILEGES ON SCHEMA metabase TO admin;
```
> ‚ö†Ô∏è Lembre-se: o lab foi desenvolvido para **localhost**, ent√£o todas as portas (8080, 8081, 8082, 8888, 9000, 9090, 3000 e 5432) devem estar livres na sua m√°quina.

---

### üìÇ Estrutura de Pastas

```bash
lab_dev_engenharia_de_dados/
‚îÇ‚îÄ‚îÄ .env                 # vari√°veis de ambiente
‚îÇ‚îÄ‚îÄ .gitignore           # arquivos/diret√≥rios ignorados pelo Git
‚îÇ‚îÄ‚îÄ docker-compose.yml   # defini√ß√£o dos servi√ßos
‚îÇ‚îÄ‚îÄ Dockerfile           # imagem customizada para o container
‚îÇ‚îÄ‚îÄ README.md            # documenta√ß√£o do projeto
‚îÇ‚îÄ‚îÄ notebooks/           # Jupyter Notebooks
‚îÇ‚îÄ‚îÄ dags/                # DAGs do Airflow
‚îÇ‚îÄ‚îÄ logs/                # logs do Airflow
‚îÇ‚îÄ‚îÄ plugins/             # plugins do Airflow
```
---

## Como rodar o ambiente

### 1Ô∏è‚É£ Clone este reposit√≥rio
Clone o reposit√≥rio e entre na pasta do projeto:  
**Comandos:**  
```bash
git clone git@github.com:FranMuniz/lab_dev_engenharia_de_dados.git 
cd lab_dev_engenharia_de_dados/
```

---

### 2Ô∏è‚É£ Configure o arquivo .env
Arquivo dispon√≠vel no reposit√≥rio, atualize com suas credenciais

---

### 3Ô∏è‚É£ Suba os servi√ßos com Docker Compose
**Comando:**  
```bash
docker-compose up -d
```

---

### 4Ô∏è‚É£ Acesse os servi√ßos

**Spark Master**  
[http://localhost:8080](http://localhost:8080)  
Interface web do Spark para acompanhar workers e jobs.

**Spark Worker**  
[http://localhost:8081](http://localhost:8081)  
Interface web do worker conectado ao cluster Spark.

**Jupyter Notebook**  
[http://localhost:8888](http://localhost:8888)  
Ambiente interativo com PySpark j√° configurado.  
N√£o √© necess√°rio informar token ou senha para acessar.

**MinIO (S3 Local)**  
[http://localhost:9090](http://localhost:9090)  
Console web para gerenciar buckets e arquivos, simulando o S3  

**Metabase**  
[http://localhost:3000](http://localhost:3000)  
Console web para visualiza√ß√£o e cria√ß√£o de dashboards com os dados do PostgreSQL  

**Airflow Webserver**  
[http://localhost:8082](http://localhost:8082)  
Interface web do Airflow para monitorar DAGs.  

**PostgreSQL**  
Host: `localhost:5432`  
Banco de dados relacional para integra√ß√£o com Spark e Airflow. 

**Airflow Scheduler**  
Respons√°vel por agendar e executar as DAGs.  

**Airflow Triggerer**  
Respons√°vel por lidar com sensores e disparos ass√≠ncronos.  
