## LaboratÃ³rio de Desenvolvimento - Engenharia de Dados

Um laboratÃ³rio completo para estudos em **Engenharia de Dados**, rodando via **Docker**.  
Aqui vocÃª encontra um ambiente integrado com **Spark, PySpark, Jupyter, PostgreSQL e Airflow**, pronto para treinar desde consultas SQL atÃ© orquestraÃ§Ã£o de pipelines de dados.

### Tecnologias

- **Apache Spark** â†’ processamento distribuÃ­do de dados.
- **PySpark** â†’ API Python para manipulaÃ§Ã£o de dados no Spark.
- **Jupyter Notebook** â†’ ambiente interativo para experimentos.
- **PostgreSQL** â†’ banco de dados relacional para integraÃ§Ã£o com Spark e Airflow.
- **Apache Airflow** â†’ orquestrador de workflows e pipelines de dados.

<p align="center">
  <img src="https://img.shields.io/badge/Spark-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/PySpark-EE4C2C?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark_SQL-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/Postgres-316192?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white"/>
</p>

---

### ðŸ“Œ Roadmap de Estudos

- [ ] Fundamentos de Spark (RDD, DataFrames, Lazy Evaluation)  
- [ ] OperaÃ§Ãµes bÃ¡sicas (`select`, `filter`, `groupBy`, `join`)  
- [ ] Leitura e escrita de dados (CSV, Parquet, PostgreSQL)  
- [ ] Spark SQL  
- [ ] IntegraÃ§Ã£o Spark + Airflow  
- [ ] Pipelines completos de Engenharia de Dados  

---

## ðŸ“‚ Estrutura de Pastas

```bash
lab_dev_engenharia_de_dados/
â”‚â”€â”€ .env                 # variÃ¡veis de ambiente
â”‚â”€â”€ .gitignore           # arquivos/diretÃ³rios ignorados pelo Git
â”‚â”€â”€ docker-compose.yml   # definiÃ§Ã£o dos serviÃ§os
â”‚â”€â”€ Dockerfile           # imagem customizada para o container
â”‚â”€â”€ README.md            # documentaÃ§Ã£o do projeto
â”‚â”€â”€ notebooks/           # Jupyter Notebooks
â”‚â”€â”€ dags/                # DAGs do Airflow
â”‚â”€â”€ logs/                # logs do Airflow
â”‚â”€â”€ plugins/             # plugins do Airflow
```
---

## Como rodar o ambiente

### 1ï¸âƒ£ Clone este repositÃ³rio
Clone o repositÃ³rio e entre na pasta do projeto:  
**Comandos:**  
```bash
git clone git@github.com:FranMuniz/lab_dev_engenharia_de_dados.git 
cd lab_dev_engenharia_de_dados/
```

---

### 2ï¸âƒ£ Configure o arquivo .env
### PostgreSQL
```
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_DB=mydb
```

### Jupyter (opcional, jÃ¡ estÃ¡ sem token)
```
JUPYTER_PASSWORD_HASH=CHAVE_HASH_AQUI
```

### Airflow
```
AIRFLOW_DB_USER=admin
AIRFLOW_DB_PASSWORD=admin
AIRFLOW_DB_NAME=airflow
AIRFLOW_FERNET_KEY=CHAVE_FERNET_AQUI
```
Gere a chave Fernet com:
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

---

### 3ï¸âƒ£ Suba os serviÃ§os com Docker Compose
**Comando:**  
```bash
docker-compose up -d
```

---

### 4ï¸âƒ£ Acesse os serviÃ§os

**Spark Master**  
[http://localhost:8080](http://localhost:8080)  
Interface web do Spark para acompanhar workers e jobs.

**Spark Worker**  
[http://localhost:8081](http://localhost:8081)  
Interface web do worker conectado ao cluster Spark.

**Jupyter Notebook**  
[http://localhost:8888](http://localhost:8888)  
Ambiente interativo com PySpark jÃ¡ configurado.  
NÃ£o Ã© necessÃ¡rio informar token ou senha para acessar.

**PostgreSQL**  
Host: `localhost:5432`  
Banco de dados relacional para integraÃ§Ã£o com Spark e Airflow.  

**Airflow Webserver**  
[http://localhost:8082](http://localhost:8082)  
Interface web do Airflow para monitorar DAGs.  

**Airflow Scheduler**  
ResponsÃ¡vel por agendar e executar as DAGs.  

**Airflow Triggerer**  
ResponsÃ¡vel por lidar com sensores e disparos assÃ­ncronos.  

---

flowchart TD
    A[ðŸ“Œ Fundamentos de Python] --> B[ðŸ“Œ Banco de Dados e SQL]
    B --> C[ðŸ“Œ Git & GitHub]
    C --> D[ðŸ“Œ ManipulaÃ§Ã£o de Dados com Python]
    D --> E[ðŸ“Œ Apache Spark & PySpark]
    E --> F[ðŸ“Œ OrquestraÃ§Ã£o com Airflow]
    F --> G[ðŸ“Œ Engenharia de Dados na PrÃ¡tica]
    G --> H[ðŸ“Œ Boas PrÃ¡ticas & Soft Skills]
    H --> I[ðŸ“Œ Extras para Diferencial]

    style A fill:#f9c74f,stroke:#000,stroke-width:1px
    style B fill:#90be6d,stroke:#000,stroke-width:1px
    style C fill:#f94144,stroke:#000,stroke-width:1px
    style D fill:#577590,stroke:#000,stroke-width:1px
    style E fill:#43aa8b,stroke:#000,stroke-width:1px
    style F fill:#f3722c,stroke:#000,stroke-width:1px
    style G fill:#577590,stroke:#000,stroke-width:1px
    style H fill:#f8961e,stroke:#000,stroke-width:1px
    style I fill:#90be6d,stroke:#000,stroke-width:1px
