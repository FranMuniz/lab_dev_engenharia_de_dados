## üíª Laborat√≥rio de Desenvolvimento - Engenharia de Dados

Um laborat√≥rio completo para estudos em **Engenharia de Dados**, rodando via **Docker**.  
Aqui voc√™ encontra um ambiente integrado com **Spark, PySpark, Jupyter, PostgreSQL e Airflow**, pronto para treinar desde consultas SQL at√© orquestra√ß√£o de pipelines de dados.

<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=mysql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/PySpark-EE4C2C?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark_SQL-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/Postgres-316192?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white"/>
  <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white"/>
</p>

---

### üìÇ Estrutura de Pastas

```bash
lab_dev_engenharia_de_dados/
‚îÇ‚îÄ‚îÄ .env                 # vari√°veis de ambiente
‚îÇ‚îÄ‚îÄ .gitignore           # arquivos/diret√≥rios ignorados pelo Git
‚îÇ‚îÄ‚îÄ docker-compose.yml   # defini√ß√£o dos servi√ßos
‚îÇ‚îÄ‚îÄ Dockerfile           # imagem customizada para o container
‚îÇ‚îÄ‚îÄ README.md            # documenta√ß√£o do projeto
‚îÇ‚îÄ‚îÄ notebooks/           # Jupyter Notebooks
‚îÇ‚îÄ‚îÄ dags/                # DAGs do Airflow
‚îÇ‚îÄ‚îÄ logs/                # logs do Airflow
‚îÇ‚îÄ‚îÄ plugins/             # plugins do Airflow
```
---

## Como rodar o ambiente

### 1Ô∏è‚É£ Clone este reposit√≥rio
Clone o reposit√≥rio e entre na pasta do projeto:  
**Comandos:**  
```bash
git clone git@github.com:FranMuniz/lab_dev_engenharia_de_dados.git 
cd lab_dev_engenharia_de_dados/
```

---

### 2Ô∏è‚É£ Configure o arquivo .env
### PostgreSQL
```
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_DB=mydb
```

### Jupyter (opcional, j√° est√° sem token)
```
JUPYTER_PASSWORD_HASH=CHAVE_HASH_AQUI
```

### Airflow
```
AIRFLOW_DB_USER=admin
AIRFLOW_DB_PASSWORD=admin
AIRFLOW_DB_NAME=airflow
AIRFLOW_FERNET_KEY=CHAVE_FERNET_AQUI
```
Gere a chave Fernet com:
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

---

### 3Ô∏è‚É£ Suba os servi√ßos com Docker Compose
**Comando:**  
```bash
docker-compose up -d
```

---

### 4Ô∏è‚É£ Acesse os servi√ßos

**Spark Master**  
[http://localhost:8080](http://localhost:8080)  
Interface web do Spark para acompanhar workers e jobs.

**Spark Worker**  
[http://localhost:8081](http://localhost:8081)  
Interface web do worker conectado ao cluster Spark.

**Jupyter Notebook**  
[http://localhost:8888](http://localhost:8888)  
Ambiente interativo com PySpark j√° configurado.  
N√£o √© necess√°rio informar token ou senha para acessar.

**PostgreSQL**  
Host: `localhost:5432`  
Banco de dados relacional para integra√ß√£o com Spark e Airflow.  

**Airflow Webserver**  
[http://localhost:8082](http://localhost:8082)  
Interface web do Airflow para monitorar DAGs.  

**Airflow Scheduler**  
Respons√°vel por agendar e executar as DAGs.  

**Airflow Triggerer**  
Respons√°vel por lidar com sensores e disparos ass√≠ncronos.  

---

### B√¥nus: Roadmap de Estudos

#### Fundamentos de Python
- [ ] Sintaxe b√°sica, vari√°veis, tipos de dados  
- [ ] Estruturas de controle (`if`, `for`, `while`)  
- [ ] Fun√ß√µes, m√≥dulos e pacotes  
- [ ] Listas, dicion√°rios, tuplas e sets  
- [ ] Manipula√ß√£o de arquivos (CSV, JSON, TXT)  
- [ ] Bibliotecas: `pandas`, `numpy`, `datetime`  

#### Banco de Dados e SQL
- [ ] Conceitos de banco de dados relacional vs n√£o-relacional  
- [ ] Cria√ß√£o de tabelas e inser√ß√£o de dados  
- [ ] Consultas b√°sicas (`SELECT`, `WHERE`, `ORDER BY`, `GROUP BY`)  
- [ ] Joins (`INNER`, `LEFT`, `RIGHT`, `FULL`)  
- [ ] Fun√ß√µes agregadas e de janela (`SUM`, `COUNT`, `ROW_NUMBER`)  
- [ ] Subqueries e CTEs (`WITH`)  
- [ ] Indexes, constraints e normaliza√ß√£o  
- [ ] Pr√°tica em PostgreSQL  

#### Git & GitHub
- [ ] Git: clone, add, commit, push, pull  
- [ ] Branches e merges  
- [ ] Pull Requests e code review  
- [ ] GitHub: reposit√≥rios, issues, GitHub Actions (CI/CD b√°sico)  

#### Manipula√ß√£o de Dados com Python
- [ ] Leitura/escrita de dados: CSV, Excel, Parquet  
- [ ] Limpeza de dados: `fillna`, `dropna`, duplicados  
- [ ] Transforma√ß√µes: `apply`, `map`, `merge`, `concat`  
- [ ] Agrega√ß√µes e pivot tables  
- [ ] Valida√ß√£o e qualidade de dados  

#### Apache Spark & PySpark
- [ ] Fundamentos de Spark (RDD, DataFrames, Lazy Evaluation)  
- [ ] Opera√ß√µes b√°sicas (`select`, `filter`, `groupBy`, `join`)  
- [ ] Leitura e escrita de dados (CSV, Parquet, PostgreSQL, S3)  
- [ ] Transforma√ß√µes avan√ßadas (`window functions`, `pivot`, `explode`)  
- [ ] Spark SQL  
- [ ] Otimiza√ß√£o: caching, partitioning, broadcast join  
- [ ] PySpark UDFs e UDAFs  

#### Orquestra√ß√£o de Pipelines (Airflow)
- [ ] DAGs e scheduling  
- [ ] Operadores: PythonOperator, BashOperator, PostgresOperator  
- [ ] Vari√°veis, XCom e templates Jinja  
- [ ] SubDAGs e modulariza√ß√£o  
- [ ] Monitoramento de pipelines e alertas  
- [ ] Integra√ß√£o Airflow + Spark + Banco de Dados  

#### Engenharia de Dados na Pr√°tica
- [ ] Pipelines de ingest√£o e transforma√ß√£o  
- [ ] Integra√ß√£o com APIs e sistemas externos  
- [ ] Processamento batch vs streaming  
- [ ] Armazenamento: bancos de dados, data lakes, warehouses  
- [ ] Logging, rastreabilidade e testes de dados  

#### Boas Pr√°ticas & Soft Skills
- [ ] Estrutura de pastas e organiza√ß√£o de c√≥digo  
- [ ] Vari√°veis de ambiente (`.env`)  
- [ ] Controle de depend√™ncias (`requirements.txt`, `pipenv`, `poetry`)  
- [ ] Documenta√ß√£o clara e README amig√°vel  
- [ ] Comunica√ß√£o com time e stakeholders  

#### Extras e Diferenciais
- [ ] Docker para ambientes isolados  
- [ ] CI/CD aplicado a pipelines de dados  
- [ ] Visualiza√ß√£o de dados: Matplotlib, Seaborn, Plotly  
- [ ] Conceitos de Big Data: Kafka, Hive, Hudi, Delta Lake  
- [ ] Aprendizado cont√≠nuo: cursos, documenta√ß√£o e projetos pr√°ticos  


