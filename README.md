## Laborat√≥rio de Desenvolvimento - Engenharia de Dados

Um laborat√≥rio completo para estudos em **Engenharia de Dados**, rodando via **Docker**.  
Aqui voc√™ encontra um ambiente integrado com **Spark, PySpark, Jupyter, PostgreSQL e Airflow**, pronto para treinar desde consultas SQL at√© orquestra√ß√£o de pipelines de dados.

### Tecnologias

- **Apache Spark** ‚Üí processamento distribu√≠do de dados.
- **PySpark** ‚Üí API Python para manipula√ß√£o de dados no Spark.
- **Jupyter Notebook** ‚Üí ambiente interativo para experimentos.
- **PostgreSQL** ‚Üí banco de dados relacional para integra√ß√£o com Spark e Airflow.
- **Apache Airflow** ‚Üí orquestrador de workflows e pipelines de dados.

<p align="center">
  <img src="https://img.shields.io/badge/Spark-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/PySpark-EE4C2C?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Spark_SQL-FF6F00?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/Postgres-316192?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white"/>
</p>

---

### üìå Roadmap de Estudos

flowchart LR
    A[üìå Python] --> B[üìå SQL]
    B --> C[üìå Git & GitHub]
    C --> D[üìå Manipula√ß√£o de Dados]
    D --> E[üìå Spark & PySpark]
    E --> F[üìå Airflow]
    F --> G[üìå PostgreSQL & Data Warehouses]
    G --> H[üìå Engenharia de Dados pr√°tica]
    H --> I[üìå Boas pr√°ticas & Soft Skills]
    I --> J[üìå Extras: Docker, CI/CD, Big Data]

    style A fill:#f9c74f,stroke:#000,stroke-width:1px
    style B fill:#90be6d,stroke:#000,stroke-width:1px
    style C fill:#f94144,stroke:#000,stroke-width:1px
    style D fill:#577590,stroke:#000,stroke-width:1px
    style E fill:#43aa8b,stroke:#000,stroke-width:1px
    style F fill:#f3722c,stroke:#000,stroke-width:1px
    style G fill:#90be6d,stroke:#000,stroke-width:1px
    style H fill:#577590,stroke:#000,stroke-width:1px
    style I fill:#f8961e,stroke:#000,stroke-width:1px
    style J fill:#f9844a,stroke:#000,stroke-width:1px

---

## üìÇ Estrutura de Pastas

```bash
lab_dev_engenharia_de_dados/
‚îÇ‚îÄ‚îÄ .env                 # vari√°veis de ambiente
‚îÇ‚îÄ‚îÄ .gitignore           # arquivos/diret√≥rios ignorados pelo Git
‚îÇ‚îÄ‚îÄ docker-compose.yml   # defini√ß√£o dos servi√ßos
‚îÇ‚îÄ‚îÄ Dockerfile           # imagem customizada para o container
‚îÇ‚îÄ‚îÄ README.md            # documenta√ß√£o do projeto
‚îÇ‚îÄ‚îÄ notebooks/           # Jupyter Notebooks
‚îÇ‚îÄ‚îÄ dags/                # DAGs do Airflow
‚îÇ‚îÄ‚îÄ logs/                # logs do Airflow
‚îÇ‚îÄ‚îÄ plugins/             # plugins do Airflow
```
---

## Como rodar o ambiente

### 1Ô∏è‚É£ Clone este reposit√≥rio
Clone o reposit√≥rio e entre na pasta do projeto:  
**Comandos:**  
```bash
git clone git@github.com:FranMuniz/lab_dev_engenharia_de_dados.git 
cd lab_dev_engenharia_de_dados/
```

---

### 2Ô∏è‚É£ Configure o arquivo .env
### PostgreSQL
```
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_DB=mydb
```

### Jupyter (opcional, j√° est√° sem token)
```
JUPYTER_PASSWORD_HASH=CHAVE_HASH_AQUI
```

### Airflow
```
AIRFLOW_DB_USER=admin
AIRFLOW_DB_PASSWORD=admin
AIRFLOW_DB_NAME=airflow
AIRFLOW_FERNET_KEY=CHAVE_FERNET_AQUI
```
Gere a chave Fernet com:
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

---

### 3Ô∏è‚É£ Suba os servi√ßos com Docker Compose
**Comando:**  
```bash
docker-compose up -d
```

---

### 4Ô∏è‚É£ Acesse os servi√ßos

**Spark Master**  
[http://localhost:8080](http://localhost:8080)  
Interface web do Spark para acompanhar workers e jobs.

**Spark Worker**  
[http://localhost:8081](http://localhost:8081)  
Interface web do worker conectado ao cluster Spark.

**Jupyter Notebook**  
[http://localhost:8888](http://localhost:8888)  
Ambiente interativo com PySpark j√° configurado.  
N√£o √© necess√°rio informar token ou senha para acessar.

**PostgreSQL**  
Host: `localhost:5432`  
Banco de dados relacional para integra√ß√£o com Spark e Airflow.  

**Airflow Webserver**  
[http://localhost:8082](http://localhost:8082)  
Interface web do Airflow para monitorar DAGs.  

**Airflow Scheduler**  
Respons√°vel por agendar e executar as DAGs.  

**Airflow Triggerer**  
Respons√°vel por lidar com sensores e disparos ass√≠ncronos.  

