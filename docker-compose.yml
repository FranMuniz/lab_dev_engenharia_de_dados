version: '3.8'

services:
  # =========================
  # POSTGRES
  # =========================
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 5s
      retries: 5
    networks:
      - datalab

  # =========================
  # SPARK MASTER
  # =========================
  spark:
    image: bitnami/spark:latest
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    networks:
      - datalab

  # =========================
  # SPARK WORKER
  # =========================
  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark
    ports:
      - "8081:8081"
    networks:
      - datalab

  # =========================
  # JUPYTER
  # =========================
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - JUPYTER_PASSWORD=${JUPYTER_PASSWORD_HASH}
    command: start-notebook.sh --NotebookApp.token=''
    depends_on:
      - spark
    networks:
      - datalab

  # =========================
  # MINIO
  # =========================
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9090:9090"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9090"
    volumes:
      - minio-data:/data
    networks:
      - datalab

  # =========================
  # METABASE
  # =========================
  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_HOST: postgres
      MB_DB_PORT: 5432
      MB_DB_DBNAME: ${POSTGRES_DB}
      MB_DB_USER: ${POSTGRES_USER}
      MB_DB_PASS: ${POSTGRES_PASSWORD}
      MB_DB_SCHEMA: metabase
    depends_on:
      - postgres
    networks:
      - datalab

  # =========================
  # AIRFLOW INIT
  # =========================
  airflow-init:
    image: apache/airflow:2.9.1-python3.11
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?options=-csearch_path%3Dairflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: utc
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: >
      bash -c "
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    networks:
      - datalab

  # =========================
  # AIRFLOW WEBSERVER
  # =========================
  airflow-webserver:
    image: apache/airflow:2.9.1-python3.11
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?options=-csearch_path%3Dairflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: utc
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8082:8080"
    command: airflow webserver
    networks:
      - datalab

  # =========================
  # AIRFLOW SCHEDULER
  # =========================
  airflow-scheduler:
    image: apache/airflow:2.9.1-python3.11
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?options=-csearch_path%3Dairflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: utc
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: airflow scheduler
    networks:
      - datalab

  # =========================
  # AIRFLOW TRIGGERER
  # =========================
  airflow-triggerer:
    image: apache/airflow:2.9.1-python3.11
    container_name: airflow-triggerer
    depends_on:
      airflow-webserver:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?options=-csearch_path%3Dairflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: utc
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: airflow triggerer
    networks:
      - datalab

# =========================
# NETWORK & VOLUMES
# =========================
networks:
  datalab:
    driver: bridge

volumes:
  postgres-db-volume:
  minio-data:

# ===========================
# URLs de acesso aos servi√ßos
# ===========================
# Spark Master: http://localhost:8080
# Spark Worker: http://localhost:8081
# Airflow Webserver: http://localhost:8082
# Jupyter Notebook: http://localhost:8888
# MinIO Console: http://localhost:9090
# MinIO API (S3): http://localhost:9000
# Metabase: http://localhost:3000
# PostgreSQL: localhost:5432